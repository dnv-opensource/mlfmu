{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Turbine Power Prediction\n",
    "\n",
    "In this study I am going to predict a wind turbine power production by using the wind speed, wind direction, month and hour data.\n",
    "\n",
    "The dataset consists of 50530 observations. In order to demonstrate my data science skills with big data, I am going to use Pyspark library.\n",
    "\n",
    "The dataset contains:\n",
    "\n",
    "* Date/Time (for 10 minutes intervals)\n",
    "* LV ActivePower (kW): The power generated by the turbine for that moment\n",
    "* Wind Speed (m/s): The wind speed at the hub height of the turbine (the wind speed that turbine use for electricity generation)\n",
    "* TheoreticalPowerCurve (KWh): The theoretical power values that the turbine generates with that wind speed which is given by the turbine manufacturer\n",
    "* Wind Direction (°): The wind direction at the hub height of the turbine (wind turbines turn to this direction automaticly)\n",
    "\n",
    "Dataset Resource: \n",
    "\n",
    "https://www.kaggle.com/berkerisen/wind-turbine-scada-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of the Study:\n",
    "\n",
    "**My aim is to predict wind turbine power production from the wind speed, wind direction, month of the year and the hour of the day.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Check for required environment variables, and set if not present\n",
    "java_home = os.getenv(\"JAVA_HOME\", \"/usr/lib/jvm/default-java\")  # General placeholder for Unix-based systems\n",
    "hadoop_home = os.getenv(\"HADOOP_HOME\", \"/usr/local/hadoop\")  # Default path for Hadoop on many Unix-based systems\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"JAVA_HOME\"] = java_home\n",
    "os.environ[\"HADOOP_HOME\"] = hadoop_home\n",
    "os.environ[\"HADOOP.HOME.DIR\"] = hadoop_home\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"PATH\"] += f\";{Path(hadoop_home) / 'bin'}\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing pyspark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Allow the user to set the Python path via environment variable or use the default system Python\n",
    "python_path = os.getenv(\"PYSPARK_PYTHON\", os.path.realpath(sys.executable))\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = python_path\n",
    "\n",
    "# Create the Spark session with dynamic configurations\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"wind_turbine_project\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.pyspark.python\", python_path)\n",
    "    .config(\"spark.pyspark.driver.python\", python_path)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset as Spark DataFrame\n",
    "spark_df = spark.read.csv(\"T1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Caching the dataset\n",
    "spark_df.cache()\n",
    "\n",
    "# Converting all the column names to lower case\n",
    "spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns])\n",
    "\n",
    "print(\"Show the first 5 rows\")\n",
    "print(spark_df.show(5))\n",
    "print()\n",
    "print(\"What are the variable data types?\")\n",
    "print(spark_df.printSchema())\n",
    "print()\n",
    "print(\"How many observations do we have?\")\n",
    "print(spark_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a substring from columns to create month and hour variables\n",
    "\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "spark_df = spark_df.withColumn(\"month\", substring(\"date/time\", 4, 2))\n",
    "spark_df = spark_df.withColumn(\"hour\", substring(\"date/time\", 12, 2))\n",
    "\n",
    "# Converting string month and hour variables to integer\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark_df = spark_df.withColumn(\"month\", spark_df.month.cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"hour\", spark_df.hour.cast(IntegerType()))\n",
    "\n",
    "print(spark_df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "spark_df.select(\"wind speed (m/s)\", \"theoretical_power_curve (kwh)\", \"lv activepower (kw)\").toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are the distributions of the variables?\n",
    "\n",
    "**For creating visualization we need to either use aggregated data or use a sample from the big data.**\n",
    "\n",
    "**So I will get a random sample from my big data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a random sample from the big data\n",
    "sample_df = spark_df.sample(withReplacement=False, fraction=0.1, seed=42).toPandas()\n",
    "\n",
    "# Visualizing the distributions with the sample data\n",
    "columns = [\n",
    "    \"wind speed (m/s)\",\n",
    "    \"wind direction (°)\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"theoretical_power_curve (kwh)\",\n",
    "    \"lv activepower (kw)\",\n",
    "]\n",
    "i = 1\n",
    "plt.figure(figsize=(10, 12))\n",
    "for each in columns:\n",
    "    plt.subplot(3, 2, i)\n",
    "    sample_df[each].plot.hist(bins=12)\n",
    "    plt.title(each)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is there any difference between the months for average power production ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average power production by month\n",
    "monthly = spark_df.groupby(\"month\").mean(\"lv activepower (kw)\").sort(\"avg(lv activepower (kw))\").toPandas()\n",
    "sns.barplot(x=\"month\", y=\"avg(lv activepower (kw))\", data=monthly)\n",
    "plt.title(\"Months and Average Power Production\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is there any difference between the hours for average power production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average power production by hour\n",
    "hourly = spark_df.groupby(\"hour\").mean(\"lv activepower (kw)\").sort(\"avg(lv activepower (kw))\").toPandas()\n",
    "sns.barplot(x=\"hour\", y=\"avg(lv activepower (kw))\", data=hourly)\n",
    "plt.title(\"Hours and Average Power Production\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is there any correlation between the wind speed, wind direction and power production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sample_df[columns].corr())\n",
    "sns.pairplot(sample_df[columns], markers=\"*\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wind speed and power production is highly correlated as one would expect.**\n",
    "\n",
    "**We can see there are lower level power production for some wind directions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the average power production level for different wind speeds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding average power production for 5 m/s wind speed increments\n",
    "wind_speed = []\n",
    "avg_power = []\n",
    "for i in [0, 5, 10, 15, 20]:\n",
    "    avg_value = (\n",
    "        spark_df.filter((spark_df[\"wind speed (m/s)\"] > i) & (spark_df[\"wind speed (m/s)\"] <= i + 5))\n",
    "        .agg({\"lv activepower (kw)\": \"mean\"})\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    avg_power.append(avg_value)\n",
    "    wind_speed.append(str(i) + \"-\" + str(i + 5))\n",
    "\n",
    "sns.barplot(x=wind_speed, y=avg_power, color=\"orange\")\n",
    "plt.title(\"Avg Power Production for 5 m/s Wind Speed Increments\")\n",
    "plt.xlabel(\"Wind Speed\")\n",
    "plt.ylabel(\"Average Power Production\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the graph above we can see the power production reaches near a maximum level after the wind speed reaches 15 m/s.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the power production for different wind directions and speeds? \n",
    "\n",
    "**Let's create a polar diagram with wind speed, wind direction and power production from the sample data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the polar diagram\n",
    "from math import radians\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "# Inside circles are the wind speed and marker color and size represents the amount of power production\n",
    "sns.scatterplot(\n",
    "    x=[radians(x) for x in sample_df[\"wind direction (°)\"]],\n",
    "    y=sample_df[\"wind speed (m/s)\"],\n",
    "    size=sample_df[\"lv activepower (kw)\"],\n",
    "    hue=sample_df[\"lv activepower (kw)\"],\n",
    "    alpha=0.7,\n",
    "    legend=None,\n",
    ")\n",
    "# Setting the polar diagram's top represents the North\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "# Setting -1 to start the wind direction clockwise\n",
    "ax.set_theta_direction(-1)\n",
    "# Setting wind speed labels in a better position to see\n",
    "ax.set_rlabel_position(110)\n",
    "plt.title(\"Wind Speed - Wind Direction - Power Production Diagram\")\n",
    "plt.ylabel(None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the wind turbine produces more power if the wind blows from the directions between 000-090 and 180-225 degrees.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Does the manufacturer's theoritical power production curve fit well with the real production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x=\"wind speed (m/s)\", y=\"lv activepower (kw)\", color=\"orange\", label=\"Real Production\", alpha=0.5, data=sample_df\n",
    ")\n",
    "sns.lineplot(\n",
    "    x=\"wind speed (m/s)\",\n",
    "    y=\"theoretical_power_curve (kwh)\",\n",
    "    color=\"blue\",\n",
    "    label=\"Theoritical Production\",\n",
    "    data=sample_df,\n",
    ")\n",
    "plt.title(\"Wind Speed and Power Production Chart\")\n",
    "plt.ylabel(\"Power Production (kw)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the graph above, we can see the theoritical power production curve generally fits well with the real production.**\n",
    "\n",
    "**We can see the power production reaches a maximum level and continues in a straight line if the wind speed reaches to 15 m/s.**\n",
    "\n",
    "**Also we can see there are some 0 power production, even the wind speed is higher than 5 m/s. I want to investigate the reason.**\n",
    "\n",
    "**But before what is the minimum wind speed for theoritical power production curve?**\n",
    "\n",
    "### Question: What is the wind speed threshold value for zero theorical power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the big data where the real and theoritical power productions are equal to 0\n",
    "zero_theo_power = spark_df.filter(\n",
    "    (spark_df[\"lv activepower (kw)\"] == 0) & (spark_df[\"theoretical_power_curve (kwh)\"] == 0)\n",
    ").toPandas()\n",
    "\n",
    "display(zero_theo_power[[\"wind speed (m/s)\", \"theoretical_power_curve (kwh)\", \"lv activepower (kw)\"]].sample(5))\n",
    "\n",
    "# Let's see the wind speed distribution for 0 power production\n",
    "zero_theo_power[\"wind speed (m/s)\"].hist()\n",
    "plt.title(\"Wind Speed Distribution for 0 Power Production\")\n",
    "plt.xlabel(\"Wind speed (m/s)\")\n",
    "plt.ylabel(\"Counts for 0 Power Production\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see from above, limit for the theoritical power curve is 3 m/s wind speed. If the wind speed is below 3 m/s model doesn't expect any power production.**\n",
    "\n",
    "**But there are some observations for 0 power production even the wind speed is more than 3 m/s.**\n",
    "\n",
    "### Question: Why there aren't any power production in some observations while the wind speed is higher than 3 m/s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations for the wind speed > 3m/s and power production = 0,\n",
    "# While theoritically there should be power production\n",
    "zero_power = spark_df.filter(\n",
    "    (spark_df[\"lv activepower (kw)\"] == 0)\n",
    "    & (spark_df[\"theoretical_power_curve (kwh)\"] != 0)\n",
    "    & (spark_df[\"wind speed (m/s)\"] > 3)\n",
    ").toPandas()\n",
    "display(zero_power.head())\n",
    "print(\"No of Observations (while Wind Speed > 3 m/s and Power Production = 0): \", len(zero_power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 3497 observations where theoritically there should be power production. From the dataset we cannot see the reason, it might be caused by maintenance. But let's see if we can see any information from the wind speed, direction and month?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_power[\"wind speed (m/s)\"].plot.hist(bins=8)\n",
    "plt.xlabel(\"Wind Speed (m/s)\")\n",
    "plt.ylabel(\"Counts for Zero Production\")\n",
    "plt.title(\"Wind Speed Counts for Zero Power Production\")\n",
    "plt.xticks(ticks=np.arange(4, 18, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It looks like theoritically wind speed threshold should be 4 m/s. But there are also other observations with zero power production while the wind speed is higher.**\n",
    "\n",
    "**Let's see the monthly distribution for zero power production.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(zero_power, x=\"month\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is usually in December and January when the wind turbine doesn't produce production.**\n",
    "\n",
    "**Because I cannot decide if these zero power productions are caused by maintenance periods or something else, I am going to accept those 3497 observations as outliers and remove them from the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the observations meeting the filter criterias\n",
    "spark_df = spark_df.filter(\n",
    "    ~(\n",
    "        (spark_df[\"lv activepower (kw)\"] == 0)\n",
    "        & (spark_df[\"theoretical_power_curve (kwh)\"] != 0)\n",
    "        & (spark_df[\"wind speed (m/s)\"] > 3)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Is there any other outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"wind speed (m/s)\", \"wind direction (°)\", \"theoretical_power_curve (kwh)\", \"lv activepower (kw)\"]\n",
    "i = 1\n",
    "plt.figure(figsize=(20, 3))\n",
    "for each in columns:\n",
    "    pandas_df = spark_df.select(each).toPandas()\n",
    "    plt.subplot(1, 4, i)\n",
    "    sns.boxplot(pandas_df)\n",
    "    plt.title(each)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the graphs above I can see there are some outliers in the wind speed data.**\n",
    "\n",
    "**I will find the upper and lower threshold values for the wind speed data, and I will analyze the outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas df for visualization\n",
    "wind_speed = spark_df.select(\"wind speed (m/s)\").toPandas()\n",
    "\n",
    "# Defining the quantiles and interquantile range\n",
    "Q1 = wind_speed[\"wind speed (m/s)\"].quantile(0.25)\n",
    "Q3 = wind_speed[\"wind speed (m/s)\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Defining the lower and upper threshold values\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "print(\"Quantile (0.25): \", Q1, \"  Quantile (0.75): \", Q3)\n",
    "print(\"Lower threshold: \", lower, \" Upper threshold: \", upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy indexing for outliers\n",
    "outlier_tf = (wind_speed[\"wind speed (m/s)\"] < lower) | (wind_speed[\"wind speed (m/s)\"] > upper)\n",
    "\n",
    "print(\"Total Number of Outliers: \", len(wind_speed[\"wind speed (m/s)\"][outlier_tf]))\n",
    "print(\"--\" * 15)\n",
    "print(\"Some Examples of Outliers:\")\n",
    "print(wind_speed[\"wind speed (m/s)\"][outlier_tf].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is a rare event for wind speed to be over 19 m/s in our dataset.**\n",
    "\n",
    "**Out of 47033, there is only 407 observations while the wind speed is over 19 m/s.**\n",
    "\n",
    "**Now I want to see average power production for these high wind speed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_df.select(\"wind speed (m/s)\", \"lv activepower (kw)\").filter(spark_df[\"wind speed (m/s)\"] >= 19).agg(\n",
    "    {\"lv activepower (kw)\": \"mean\"}\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So instead of erasing the outliers, I am going to set the wind speed as 19 m/s for those observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"wind speed (m/s)\", f.when(f.col(\"wind speed (m/s)\") > 19.447, 19).otherwise(f.col(\"wind speed (m/s)\"))\n",
    ")\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are the general criterias for power production?\n",
    "\n",
    "**It is important to understand the pattern in the data. We should learn the data before the machine.**\n",
    "\n",
    "**1. We saw from the graph that in March, August and November, the average power production is higher.**\n",
    "\n",
    "**2. The average power production is higher daily between 16:00 and 24:00.**\n",
    "\n",
    "**3. The power production is higher when the wind blows from the directions between 000-090 and 180-225 degrees.**\n",
    "\n",
    "**So let's try to predict a high and low level of power production from the criterias above before ML algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High level power production\n",
    "spark_df.filter(\n",
    "    ((spark_df[\"month\"] == 3) | (spark_df[\"month\"] == 8) | (spark_df[\"month\"] == 11))\n",
    "    & ((spark_df[\"hour\"] >= 16) | (spark_df[\"hour\"] <= 24))\n",
    "    & ((spark_df[\"wind direction (°)\"] > 0) | (spark_df[\"wind direction (°)\"] < 90))\n",
    "    & ((spark_df[\"wind direction (°)\"] > 180) | (spark_df[\"wind direction (°)\"] < 225))\n",
    ").agg({\"lv activepower (kw)\": \"mean\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low level power production\n",
    "spark_df.filter(\n",
    "    (spark_df[\"month\"] == 7)\n",
    "    & ((spark_df[\"hour\"] >= 9) | (spark_df[\"hour\"] <= 11))\n",
    "    & ((spark_df[\"wind direction (°)\"] > 90) | (spark_df[\"wind direction (°)\"] < 160))\n",
    ").agg({\"lv activepower (kw)\": \"mean\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for ML Algorithms\n",
    "\n",
    "**After analysing and understanding the dataset, we can build a ML regression model to predict wind turbine power production by using the wind speed, wind direction, month of the year and hour of the day.**\n",
    "\n",
    "**Using ML algorithms with Spark is a bit different from well known Sckitlearn library.**\n",
    "\n",
    "**We need to feed the model with a dataframe made of variables compressed in vectors called as 'features', and target variable as 'label'. For these convertions I am going to use VectorAssembler method from Pyspark.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the independent variables (Features)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Converting lv activepower (kw) variable as label\n",
    "spark_df = spark_df.withColumn(\"label\", spark_df[\"lv activepower (kw)\"])\n",
    "\n",
    "# Defining the variables to be used\n",
    "variables = [\"month\", \"hour\", \"wind speed (m/s)\", \"wind direction (°)\"]\n",
    "vector_assembler = VectorAssembler(inputCols=variables, outputCol=\"features\")\n",
    "va_df = vector_assembler.transform(spark_df)\n",
    "\n",
    "# Combining features and label column\n",
    "final_df = va_df.select(\"features\", \"label\")\n",
    "final_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "**Now we can split our dataset into train and test datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = final_df.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "\n",
    "print(\"Train dataset: \", train_df.count())\n",
    "print(\"Test dataset : \", test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Initial Model\n",
    "\n",
    "**I am going to use Random Forest Regressor for this study.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Training the model with train data\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Predicting using the test data\n",
    "y_pred = rf_model.transform(test_df)\n",
    "\n",
    "# Initial look at the target and predicted values\n",
    "y_pred.select(\"label\", \"prediction\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's evaluate our model's success.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model success\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "\n",
    "print(\"R2 SCORE : \", evaluator.evaluate(y_pred, {evaluator.metricName: \"r2\"}))\n",
    "print(\"MAE      : \", evaluator.evaluate(y_pred, {evaluator.metricName: \"mae\"}))\n",
    "print(\"RMSE     : \", evaluator.evaluate(y_pred, {evaluator.metricName: \"rmse\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2 score means, real power production's 97% variability can be explained by the ML model.**\n",
    "\n",
    "**MAE is the mean absolute difference between the real and predicted power production.**\n",
    "\n",
    "**RMSE is the square root of mean squared difference between the real and predicted values.**\n",
    "\n",
    "**Even though the R2 is high, we should also check the MAE and RMSE values with the real value's summary statistics.**\n",
    "\n",
    "**One can tune the hyperparameters to increase the model success. But I this look good enough for me.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Real, Theoritical and Predicted Power Productions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am going to use sample_df for comparing the actual, theoritical and the model power productions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am going to use sample_df for comparing the actual, theoritical and the model power productions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting sample_df back to Spark dataframe\n",
    "eva_df = spark.createDataFrame(sample_df)\n",
    "\n",
    "# Converting lv activepower (kw) variable as label\n",
    "eva_df = eva_df.withColumn(\"label\", eva_df[\"lv activepower (kw)\"])\n",
    "\n",
    "# Defining the variables to be used\n",
    "variables = [\"month\", \"hour\", \"wind speed (m/s)\", \"wind direction (°)\"]\n",
    "vector_assembler = VectorAssembler(inputCols=variables, outputCol=\"features\")\n",
    "vec_df = vector_assembler.transform(eva_df)\n",
    "\n",
    "# Combining features and label column\n",
    "vec_df = vec_df.select(\"features\", \"label\")\n",
    "\n",
    "# Using ML model to predict\n",
    "preds = rf_model.transform(vec_df)\n",
    "preds_df = preds.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "# Compining dataframes to compare\n",
    "frames = [sample_df[[\"wind speed (m/s)\", \"theoretical_power_curve (kwh)\"]], preds_df]\n",
    "sample_data = pd.concat(frames, axis=1)\n",
    "\n",
    "# Visualizing real, theoritical and predicted power production\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=\"wind speed (m/s)\", y=\"label\", alpha=0.5, label=\"Real Power\", data=sample_data)\n",
    "sns.scatterplot(x=\"wind speed (m/s)\", y=\"prediction\", alpha=0.7, label=\"Predicted Power\", marker=\"o\", data=sample_data)\n",
    "sns.lineplot(\n",
    "    x=\"wind speed (m/s)\", y=\"theoretical_power_curve (kwh)\", label=\"Theoritical Power\", color=\"purple\", data=sample_data\n",
    ")\n",
    "plt.title(\"Wind Turbine Power Production Prediction\")\n",
    "plt.ylabel(\"Power Production (kw)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the graph above, the model fits better to the real power productions, than the theoritical power production curve.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting RF Regressor Model as an ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx.defs import onnx_opset_version\n",
    "from onnxconverter_common.onnx_ex import DEFAULT_OPSET_NUMBER\n",
    "from onnxmltools import convert_sparkml\n",
    "from onnxmltools.convert.common.data_types import FloatTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_OPSET = min(DEFAULT_OPSET_NUMBER, onnx_opset_version())\n",
    "\n",
    "# Define the input types - 'features' is the input column, len(variables) is the number of features (4 in your case)\n",
    "variables = [\"month\", \"hour\", \"wind speed (m/s)\", \"wind direction (°)\"]\n",
    "initial_types = [(\"features\", FloatTensorType([1, len(variables)]))]\n",
    "\n",
    "onnx_model = convert_sparkml(\n",
    "    rf_model, \"Random Forest Regressor Model\", initial_types, spark_session=spark, target_opset=TARGET_OPSET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the onnx model structure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = onnx_model.graph\n",
    "\n",
    "# Inspect inputs\n",
    "print(\"Inputs:\")\n",
    "for inp in graph.input:\n",
    "    print(f\"Name: {inp.name}, Shape: {inp.type.tensor_type.shape}, Type: {inp.type.tensor_type.elem_type}\")\n",
    "\n",
    "# Inspect outputs\n",
    "print(\"\\nOutputs:\")\n",
    "for out in graph.output:\n",
    "    print(f\"Name: {out.name}, Shape: {out.type.tensor_type.shape}, Type: {out.type.tensor_type.elem_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exporting the onnx model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ONNX model to a file\n",
    "with Path(\"rf_model.onnx\").open(\"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the onnx model with some input values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "# Load the ONNX model\n",
    "sess = rt.InferenceSession(\"rf_model.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# Prepare input as a numpy array (for example)\n",
    "input_data = np.array(\n",
    "    [[1.0, 0.0, 5.31133604049682, 259.994903564453]], dtype=np.float32\n",
    ")  # month, hour, wind speed, wind direction\n",
    "\n",
    "# Run the model\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "predictions = sess.run([label_name], {input_name: input_data})\n",
    "\n",
    "print(\"Predicted value:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can create an FMU model using MLFMU package.**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 133415,
     "sourceId": 317642,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30019,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mlfmu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
